### NLP Papers

#### Embedding
- [BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- [EMLO:Deep contextualized word representations](https://arxiv.org/abs/1802.05365)
  
  With bi-directional LSTM model based on character-level CNN and aggregation 3-layers representation for each word, as 2-side model could learn word representation more powerful, also for each word will get different representation based on left and right word, so it could learn more than just one fixed word representation.