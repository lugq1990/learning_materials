### Activation Papers

#### Activation
- [GELU: GAUSSIAN ERROR LINEAR UNITS](https://arxiv.org/pdf/1606.08415.pdf)
  
  Increase Non-linear ability with input * cumulative distribution function of normal distribution as neural output trend to be normal with batch normalization.

  `0.5x(1 + tanh[sqrt(2/Ï€)(x + 0.044715x3)])` or `x*sigmoid(1.702x)`

  ![function graph](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_12.48.44_PM.png)